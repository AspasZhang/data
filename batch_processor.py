"""
æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜çš„è„šæœ¬
åªéœ€ä¿®æ”¹questionså’Œknowledge_basesä¸¤ä¸ªåˆ—è¡¨å³å¯
"""

import sys
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, '/mnt/user-data/outputs')

from tool_manager import ToolManager
from agent_generator import AgentGenerator


# ============================================================
# ğŸ“ åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„é—®é¢˜å’ŒçŸ¥è¯†åº“åˆ—è¡¨
# ============================================================

# é—®é¢˜åˆ—è¡¨
QUESTIONS = [
    "è¯·åˆ†æè®¾å¤‡aggrleaf02_2_20.45çš„æ¥å£æ˜¯å¦æœ‰å¼‚å¸¸çŠ¶æ€ï¼Œå¹¶å®šä½æ•…éšœåŸå› ï¼Ÿ",
    "serverleaf01_1_16.135è®¾å¤‡ä¸Š10GE1/0/24æ¥å£å‘ç”Ÿä¸¢åŒ…è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ",
    "è®¾å¤‡spine01çš„BGPé‚»å±…å…³ç³»å¼‚å¸¸ï¼Œè¯·å¸®æˆ‘æ’æŸ¥é—®é¢˜ã€‚",
    # åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé—®é¢˜...
]

# çŸ¥è¯†åº“åˆ—è¡¨ï¼ˆå¿…é¡»å’Œé—®é¢˜åˆ—è¡¨ä¸€ä¸€å¯¹åº”ï¼Œæˆ–è€…åªæœ‰ä¸€ä¸ªè¡¨ç¤ºæ‰€æœ‰é—®é¢˜å…±ç”¨ï¼‰
KNOWLEDGE_BASES = [
    "/mnt/user-data/uploads/workflow.json",
    "/mnt/user-data/uploads/workflow.json",
    "/mnt/user-data/uploads/workflow.json",
    # åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šçŸ¥è¯†åº“è·¯å¾„...
]

# ============================================================
# è¿è¡Œé…ç½®ï¼ˆå¯é€‰ä¿®æ”¹ï¼‰
# ============================================================

# æ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šå°‘æ¡æ•°æ®
N_RUNS = 10

# æœ€å¤§æ­¥éª¤æ•°
MAX_STEPS = 20

# æ˜¯å¦æ”¹å†™é—®é¢˜ä»¥å¢åŠ å¤šæ ·æ€§
REWRITE_QUESTION = True

# å·¥å…·æ–‡ä»¶è·¯å¾„
TOOLS_FILE = '/mnt/user-data/outputs/available_tools_with_params.txt'

# è¾“å‡ºåŸºç¡€ç›®å½•
OUTPUT_BASE_DIR = '/mnt/user-data/outputs/batch_results'

# APIé…ç½®
API_KEY = "kw-qIdb2KBfLLBkk6YEJ1clWKKOctnHgWMjtfRJwQ2yTLBCXjMv"
API_BASE = "http://10.12.208.86:8502"

# ============================================================
# ä¸»å¤„ç†é€»è¾‘ï¼ˆä¸éœ€è¦ä¿®æ”¹ï¼‰
# ============================================================


def validate_inputs():
    """éªŒè¯è¾“å…¥é…ç½®"""
    if not QUESTIONS:
        raise ValueError("âŒ QUESTIONSåˆ—è¡¨ä¸èƒ½ä¸ºç©ºï¼")
    
    if not KNOWLEDGE_BASES:
        raise ValueError("âŒ KNOWLEDGE_BASESåˆ—è¡¨ä¸èƒ½ä¸ºç©ºï¼")
    
    # æ£€æŸ¥æ•°é‡åŒ¹é…
    if len(KNOWLEDGE_BASES) == 1:
        print(f"â„¹ï¸  æ‰€æœ‰ {len(QUESTIONS)} ä¸ªé—®é¢˜å°†ä½¿ç”¨åŒä¸€ä¸ªçŸ¥è¯†åº“")
        return [(q, KNOWLEDGE_BASES[0]) for q in QUESTIONS]
    elif len(QUESTIONS) == len(KNOWLEDGE_BASES):
        print(f"â„¹ï¸  {len(QUESTIONS)} ä¸ªé—®é¢˜å°†åˆ†åˆ«ä½¿ç”¨å¯¹åº”çš„çŸ¥è¯†åº“")
        return list(zip(QUESTIONS, KNOWLEDGE_BASES))
    else:
        raise ValueError(
            f"âŒ é—®é¢˜æ•°é‡({len(QUESTIONS)})å’ŒçŸ¥è¯†åº“æ•°é‡({len(KNOWLEDGE_BASES)})ä¸åŒ¹é…ï¼"
            f"\n   çŸ¥è¯†åº“åˆ—è¡¨å¿…é¡»æ˜¯1ä¸ªï¼ˆå…±ç”¨ï¼‰æˆ–ä¸é—®é¢˜æ•°é‡ç›¸åŒ"
        )


def load_knowledge_base(kb_path: str) -> dict:
    """åŠ è½½çŸ¥è¯†åº“"""
    try:
        with open(kb_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"   âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
        print(f"   â„¹ï¸  å°†ä½¿ç”¨é»˜è®¤é…ç½®")
        return {}


def process_question(question: str, kb_path: str, question_idx: int, 
                     tool_manager: ToolManager, output_base: Path):
    """
    å¤„ç†å•ä¸ªé—®é¢˜
    
    Args:
        question: é—®é¢˜æè¿°
        kb_path: çŸ¥è¯†åº“è·¯å¾„
        question_idx: é—®é¢˜ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
        tool_manager: å·¥å…·ç®¡ç†å™¨
        output_base: è¾“å‡ºåŸºç¡€ç›®å½•
    """
    print("\n" + "=" * 80)
    print(f"ğŸ“„ å¤„ç†é—®é¢˜ {question_idx}/{len(QUESTIONS)}")
    print("=" * 80)
    print(f"é—®é¢˜: {question[:80]}{'...' if len(question) > 80 else ''}")
    print(f"çŸ¥è¯†åº“: {kb_path}")
    print("=" * 80 + "\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    question_dir = output_base / f"question_{question_idx:03d}"
    question_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜é—®é¢˜ä¿¡æ¯
    question_info = {
        "question_id": question_idx,
        "question": question,
        "knowledge_base": kb_path,
        "timestamp": datetime.now().isoformat(),
        "n_runs": N_RUNS,
        "max_steps": MAX_STEPS
    }
    
    with open(question_dir / "question_info.json", 'w', encoding='utf-8') as f:
        json.dump(question_info, f, ensure_ascii=False, indent=2)
    
    # åŠ è½½çŸ¥è¯†åº“
    print("ğŸ“š åŠ è½½çŸ¥è¯†åº“...")
    knowledge_base = load_knowledge_base(kb_path)
    print()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    print("ğŸ¤– åˆå§‹åŒ–Agentç”Ÿæˆå™¨...")
    generator = AgentGenerator(
        tool_manager=tool_manager,
        api_key=API_KEY,
        api_base=API_BASE,
        knowledge_base=knowledge_base,
        max_steps=MAX_STEPS
    )
    print()
    
    # æ‰¹é‡ç”Ÿæˆ
    print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {N_RUNS} æ¡æ•°æ®...")
    print("-" * 80 + "\n")
    
    try:
        results = generator.generate_batch(
            question=question,
            n_runs=N_RUNS,
            output_dir=str(question_dir),
            rewrite_question=REWRITE_QUESTION
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print("\n" + "=" * 80)
        print(f"âœ… é—®é¢˜ {question_idx} ç”Ÿæˆå®Œæˆï¼")
        print("=" * 80)
        print(f"è¾“å‡ºç›®å½•: {question_dir}")
        print(f"ç”Ÿæˆæ–‡ä»¶:")
        print(f"  - å•æ¬¡è¿è¡Œ: run_*.json ({N_RUNS}ä¸ª)")
        print(f"  - æ‰¹é‡æ±‡æ€»: batch_summary.json")
        print(f"  - é—®é¢˜ä¿¡æ¯: question_info.json")
        
        # è·¯å¾„å¤šæ ·æ€§åˆ†æ
        def extract_path(result):
            """ä»æ–°æ ¼å¼ä¸­æå–è·¯å¾„"""
            steps = result.get('response', [])
            path = []
            for step_dict in steps:
                for step_key, step_data in step_dict.items():
                    coa = step_data.get('coa', [])
                    for action_obs in coa:
                        tool_name = action_obs.get('action', {}).get('name')
                        if tool_name:
                            path.append(tool_name)
            return tuple(path)
        
        paths = [extract_path(r) for r in results]
        unique_paths = len(set(paths))
        
        print(f"\nğŸ“Š è·¯å¾„å¤šæ ·æ€§:")
        print(f"  æ€»è¿è¡Œæ•°: {len(results)}")
        print(f"  å”¯ä¸€è·¯å¾„: {unique_paths}")
        print(f"  å¤šæ ·æ€§æ¯”ä¾‹: {unique_paths/len(results)*100:.1f}%")
        
        # æ˜¾ç¤ºå‰3æ¡è·¯å¾„
        if len(results) > 0:
            print(f"\nå‰3æ¡è·¯å¾„ç¤ºä¾‹:")
            for i, result in enumerate(results[:3], 1):
                path = list(extract_path(result))
                steps = len(path)
                print(f"  Run {i} ({steps}æ­¥): {' â†’ '.join(path[:5])}" + 
                      (f" â†’ ..." if steps > 5 else ""))
        
        print("=" * 80)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ é—®é¢˜ {question_idx} ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ æ‰¹é‡é—®é¢˜å¤„ç†ç³»ç»Ÿ")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n")
    
    # éªŒè¯è¾“å…¥
    print("ğŸ“‹ éªŒè¯é…ç½®...")
    try:
        question_kb_pairs = validate_inputs()
    except ValueError as e:
        print(str(e))
        return
    
    print(f"âœ… é…ç½®éªŒè¯é€šè¿‡")
    print(f"   é—®é¢˜æ€»æ•°: {len(QUESTIONS)}")
    print(f"   æ¯é—®é¢˜è¿è¡Œ: {N_RUNS} æ¬¡")
    print(f"   æ€»æ•°æ®æ¡æ•°: {len(QUESTIONS) * N_RUNS}")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_BASE_DIR}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_base = Path(OUTPUT_BASE_DIR)
    output_base.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨ï¼ˆæ‰€æœ‰é—®é¢˜å…±äº«ï¼‰
    print("ğŸ“‹ æ­¥éª¤1: åŠ è½½å·¥å…·åˆ—è¡¨...")
    tool_manager = ToolManager(TOOLS_FILE)
    print(f"   âœ… å·²åŠ è½½ {len(tool_manager.tools)} ä¸ªå·¥å…·\n")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    success_count = 0
    failed_questions = []
    
    for idx, (question, kb_path) in enumerate(question_kb_pairs, 1):
        success = process_question(
            question=question,
            kb_path=kb_path,
            question_idx=idx,
            tool_manager=tool_manager,
            output_base=output_base
        )
        
        if success:
            success_count += 1
        else:
            failed_questions.append(idx)
    
    # ç”Ÿæˆæ€»ä½“æ±‡æ€»
    print("\n\n" + "=" * 80)
    print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print(f"æ€»é—®é¢˜æ•°: {len(QUESTIONS)}")
    print(f"æˆåŠŸå¤„ç†: {success_count}")
    print(f"å¤±è´¥å¤„ç†: {len(failed_questions)}")
    if failed_questions:
        print(f"å¤±è´¥é—®é¢˜ç´¢å¼•: {failed_questions}")
    print(f"æ¯é—®é¢˜ç”Ÿæˆ: {N_RUNS} æ¡")
    print(f"æ€»æ•°æ®æ¡æ•°: {success_count * N_RUNS}")
    print(f"è¾“å‡ºç›®å½•: {output_base}")
    
    # ä¿å­˜æ€»ä½“æ±‡æ€»
    summary_file = output_base / "all_questions_summary.json"
    summary_data = {
        "total_questions": len(QUESTIONS),
        "successful": success_count,
        "failed": len(failed_questions),
        "failed_indices": failed_questions,
        "n_runs_per_question": N_RUNS,
        "max_steps": MAX_STEPS,
        "timestamp": datetime.now().isoformat(),
        "questions": [
            {
                "question_id": i,
                "question": q,
                "knowledge_base": kb,
                "output_dir": str(output_base / f"question_{i:03d}")
            }
            for i, (q, kb) in enumerate(question_kb_pairs, 1)
        ]
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ€»ä½“æ±‡æ€»å·²ä¿å­˜: {summary_file}")
    print("=" * 80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)


if __name__ == '__main__':
    main()
